{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN-RL",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPC1kWDTU/X/4Z8SKGzp0yV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/h-gokul/ReinforcementLearningBasics/blob/master/DQN_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL5r2XJDWmS2",
        "colab_type": "text"
      },
      "source": [
        "# Deep Q Learning networks \n",
        "  Every reward and state achieved by a agent in the environement is the *function* of the action performed by the agent in that state of the environment. This function was usually mapped with a Q table that evaluates the Q value, regularly updated - thereby learnt, for every sequence of actions performed. - This is Q learning.\n",
        "\n",
        "  In Reinforcement Learning, the idea of implementing Q learning and storing them in a Q table can be optimal and viable for small problems. But desigining a LUT for a complex environment and agent requires multitudes of data to be stored in 'actionspace x observationspace' dimensions. Thus, by replacing a Q table with a neural network to obtain this *function* for any action performed, we manage to save data and improve accuracy.   \n",
        "\n",
        " The notebook explores a DQN version of the  player vs food vs enemy  Blob game which was previously done in Q-table method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2scIPmDWk3H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56962d6d-6099-4d95-8ab7-6b3172795121"
      },
      "source": [
        "import numpy as np\n",
        "import keras.backend.tensorflow_backend as backend\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import TensorBoard\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "import time\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "\n",
        "DISCOUNT = 0.99\n",
        "REPLAY_MEMORY_SIZE = 50_000  # How many last steps to keep for model training\n",
        "MIN_REPLAY_MEMORY_SIZE = 1_000  # Minimum number of steps in a memory to start training\n",
        "MINIBATCH_SIZE = 64  # How many steps (samples) to use for training\n",
        "UPDATE_TARGET_EVERY = 5  # Terminal states (end of episodes)\n",
        "MODEL_NAME = '2x256'\n",
        "MIN_REWARD = -200  # For model save\n",
        "MEMORY_FRACTION = 0.20\n",
        "\n",
        "# Environment settings\n",
        "EPISODES = 20_000\n",
        "\n",
        "# Exploration settings\n",
        "epsilon = 1  # not a constant, going to be decayed\n",
        "EPSILON_DECAY = 0.99975\n",
        "MIN_EPSILON = 0.001\n",
        "\n",
        "#  Stats settings\n",
        "AGGREGATE_STATS_EVERY = 50  # episodes\n",
        "SHOW_PREVIEW = False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN02PCOiV-TT",
        "colab_type": "text"
      },
      "source": [
        "In every episode, every step we take, we want to update Q values, but we also are trying to predict from our model.\n",
        "Especially initially, our model is starting off as random, and it's being updated every single step, per every single episode.\n",
        "What ensues here are massive fluctuations that are super confusing to our model. \n",
        "This is why we almost always train neural networks with batches (that and the time-savings).\n",
        "One way this is solved is through a concept of memory replay, whereby we actually have two models. 'model' is the actual model that we are training.\n",
        "'target_model' is updated every n episodes(a parameter to be decided)\n",
        "Eventually, we converge the two models so they are the same, but we want the model that we query for future Q values to be more stable than the model that we're actively fitting every single step.\n",
        "###### Replay Memory \n",
        "- This concept is implemented to avoid fluctuations that occur due to extensive number of .fit() and .predict() that happen every iteration every episode with single sample training.\n",
        "-  Thus, we can maintain  a memory for our agent, that remembers the previous say 1000 actions and train as minibatches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DDDnoC5-UFyi"
      },
      "source": [
        "# The DQN Agent routine\n",
        "\n",
        "We initialise a DQN agent, which uses two models to  gradually update a final 'model' in minibatches. The minibatch from the replay memory already has a set of states, actions and the corresponding rewards stored in it. The Model is a 2 layer 2D Convnet model. It's input will be the observation space - (say 10x10x9) and its output will be actions (9 actions in this case). Th model is trained as follows\n",
        "\n",
        "- If the replay memory is large enough, - begin training.\n",
        "#### Process the Q values for every action in a batch  \n",
        "- Random sample a minibatch (size 64x5) from the replay memory (REPLAYMEMORY_SIZE x 5) : 64 = MINIBATCH_SIZE; 5 = len(currentstates,action,reward,newstates,done). Current states and new states are images.\n",
        "- Obtain list of current states from the batch, perform the prediction in 'model' to obtain list of current Q values.\n",
        "- Obtain list of new states from the batch , perform the prediction in 'target model' to obtain list of future Q values (this is not new Q values. Remember new Q can be found only by the optimization learning function).\n",
        "- if the routine is not done, calculate maxfutureQ = max of the list (future Q values). Using maxfutureq and the reward, find the newQ for that action.  Else, where it will be 'done', there is not maxfutureQ so newQ=reward for that action.\n",
        "- For a given action, update the corresponding Q value from the currentqlist with the new q values.  \n",
        "- Repeat this process through the whole batch. Now we have  a batch with updated Q values for every action performed resulted from the previous model prediction\n",
        "#### Train the model.\n",
        "-   'X' - Current state images  'y' -current q values after action-wise updation \n",
        " \n",
        " \n",
        "The 'model' is trained with X and Y for every iteration while 'target model' obtains the weights of 'model' 'UPDATE_TARGET_EVERY' iterations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nJfOd9UBUFyQ",
        "colab": {}
      },
      "source": [
        "class DQNAgent:\n",
        "    \n",
        "    def __init__(self):\n",
        "\n",
        "        # main model  # gets trained every step\n",
        "        self.model = self.create_model()\n",
        "\n",
        "        # Target model this is what we .predict against every step\n",
        "        self.target_model = self.create_model()\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE) # This memory variable can be dynamically extended from both sides\n",
        "        self.tensorboard = ModifiedTensorBoard(log_dir=f\"logs/{MODEL_NAME}-{int(time.time())}\") # \n",
        "        self.target_update_counter = 0 \n",
        "\n",
        "    \n",
        "    # A simple Conv2D 2 layer neural net    \n",
        "    def create_model(self):\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(256, (3, 3), input_shape=env.OBSERVATION_SPACE_VALUES))  # OBSERVATION_SPACE_VALUES = (10, 10, 3) a 10x10 RGB image.\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "        model.add(Conv2D(256, (3, 3)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "        model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "        model.add(Dense(64))\n",
        "\n",
        "        model.add(Dense(env.ACTION_SPACE_SIZE, activation='linear'))  # ACTION_SPACE_SIZE = how many choices (9)\n",
        "        model.compile(loss=\"mse\", optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "     # Updates the replay memory, with the below values.\n",
        "    # (observation space (current states), action, reward, new observation space(new states), done)\n",
        "    def update_replay_memory(self, transition):\n",
        "        self.replay_memory.append(transition)\n",
        "    \n",
        "    \n",
        "    # to obtain Q values, which is to perform a .predict() \n",
        "    def get_qs(self, state):\n",
        "        return self.model.predict(np.array(state).reshape(-1, *state.shape)/255)[0] # reshape is done for shape compatibility.\n",
        "\n",
        "    def train(self, terminal_state, step):\n",
        "        \n",
        "        # Start training only if certain number of samples is already saved\n",
        "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
        "            return\n",
        "        # Obtain a minibatch of size MINIBATCH_SIZE from the replay_memory\n",
        "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
        "\n",
        "        # Obtain current states from the 'minbatch' \n",
        "        current_states = np.array([transition[0] for transition in minibatch])/255  # transistion[0] is the index of current states   \n",
        "        current_qs_list = self.model.predict(current_states) # q value is obtained by a model. predict operation\n",
        "\n",
        "        # Obtain new states from the 'minbatch' \n",
        "        new_current_states = np.array([transition[3] for transition in minibatch])/255  # transistion[3] is the index of current states   \n",
        "        future_qs_list = self.target_model.predict(new_current_states) # q value is obtained by a model. predict operation\n",
        "\n",
        "        ## Preparing the X and y to train the model\n",
        "        X = []\n",
        "        y = []\n",
        "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
        "\n",
        "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
        "            # almost like with Q Learning, but we use just part of equation here\n",
        "            if not done:\n",
        "                max_future_q = np.max(future_qs_list[index])\n",
        "                new_q = reward + DISCOUNT * max_future_q\n",
        "            else:\n",
        "                new_q = reward\n",
        "\n",
        "            # Update Q value for given state\n",
        "            current_qs = current_qs_list[index]\n",
        "            current_qs[action] = new_q\n",
        "\n",
        "            # And append to our training data\n",
        "            X.append(current_state)\n",
        "            y.append(current_qs)\n",
        "\n",
        "\n",
        "        # Fit on all samples as one batch, log only on terminal state\n",
        "        self.model.fit(np.array(X)/255, np.array(y), batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False, callbacks=[self.tensorboard] if terminal_state else None)\n",
        "\n",
        "        if terminal_state:\n",
        "          self.target_update_counter+=1\n",
        "\n",
        "        # If counter reaches set value, update target network with weights of main network\n",
        "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
        "            self.target_model.set_weights(self.model.get_weights())\n",
        "            self.target_update_counter = 0\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HrMb5Bu1UFxn",
        "colab": {}
      },
      "source": [
        "# Since the .fit creates a log file every time it is called, we use the tensorboard class to skip unnecessary logging during this extensive training program.\n",
        "\n",
        "class ModifiedTensorBoard(TensorBoard):\n",
        "\n",
        "    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.step = 1\n",
        "        self.writer = tf.summary.create_file_writer(self.log_dir)\n",
        "\n",
        "    # Overriding this method to stop creating default log writer\n",
        "    def set_model(self, model):\n",
        "        pass\n",
        "\n",
        "    # Overrided, saves logs with our step number\n",
        "    # (otherwise every .fit() will start writing from 0th step)\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.update_stats(**logs)\n",
        "\n",
        "    # Overrided\n",
        "    # We train for one batch only, no need to save anything at epoch end\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        pass\n",
        "\n",
        "    # Overrided, so won't close writer\n",
        "    def on_train_end(self, _):\n",
        "        pass\n",
        "\n",
        "    # Custom method for saving own metrics\n",
        "    # Creates writer, writes custom metrics and closes writer\n",
        "    def update_stats(self, **stats):\n",
        "        self._write_logs(stats, self.step)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHAkC3ONTXU_",
        "colab_type": "text"
      },
      "source": [
        "# The participants\n",
        "\n",
        "Blob components can be a food, player or an enemy. It can move in 9 directions. i.e 9 actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3oMWUEoQP5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Blob:\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "        self.x = np.random.randint(0, size)\n",
        "        self.y = np.random.randint(0, size)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"Blob ({self.x}, {self.y})\"\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        return (self.x-other.x, self.y-other.y)\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        return self.x == other.x and self.y == other.y\n",
        "\n",
        "    def action(self, choice):\n",
        "        '''\n",
        "        Gives us 9 total movement options. (0,1,2,3,4,5,6,7,8)\n",
        "        '''\n",
        "        if choice == 0:\n",
        "            self.move(x=1, y=1)\n",
        "        elif choice == 1:\n",
        "            self.move(x=-1, y=-1)\n",
        "        elif choice == 2:\n",
        "            self.move(x=-1, y=1)\n",
        "        elif choice == 3:\n",
        "            self.move(x=1, y=-1)\n",
        "\n",
        "        elif choice == 4:\n",
        "            self.move(x=1, y=0)\n",
        "        elif choice == 5:\n",
        "            self.move(x=-1, y=0)\n",
        "\n",
        "        elif choice == 6:\n",
        "            self.move(x=0, y=1)\n",
        "        elif choice == 7:\n",
        "            self.move(x=0, y=-1)\n",
        "\n",
        "        elif choice == 8:\n",
        "            self.move(x=0, y=0)\n",
        "\n",
        "    def move(self, x=False, y=False):\n",
        "\n",
        "        # If no value for x, move randomly (In case of food and enemy)\n",
        "        if not x:\n",
        "            self.x += np.random.randint(-1, 2)\n",
        "        else:\n",
        "            self.x += x\n",
        "\n",
        "        # If no value for y, move randomly (In case of food and enemy)\n",
        "        if not y:\n",
        "            self.y += np.random.randint(-1, 2)\n",
        "        else:\n",
        "            self.y += y\n",
        "\n",
        "        # If we are out of bounds, fix!\n",
        "        if self.x < 0:\n",
        "            self.x = 0\n",
        "        elif self.x > self.size-1:\n",
        "            self.x = self.size-1\n",
        "        if self.y < 0:\n",
        "            self.y = 0\n",
        "        elif self.y > self.size-1:\n",
        "            self.y = self.size-1\n",
        "          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcrknIBNURsS",
        "colab_type": "text"
      },
      "source": [
        "# The Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HK1tB7wTUQWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BlobEnv:\n",
        "    SIZE = 10\n",
        "    RETURN_IMAGES = True\n",
        "    MOVE_PENALTY = 1\n",
        "    ENEMY_PENALTY = 300\n",
        "    FOOD_REWARD = 25\n",
        "    OBSERVATION_SPACE_VALUES = (SIZE, SIZE, 3)  # 4\n",
        "    ACTION_SPACE_SIZE = 9\n",
        "    PLAYER_N = 1  # player key in dict\n",
        "    FOOD_N = 2  # food key in dict\n",
        "    ENEMY_N = 3  # enemy key in dict\n",
        "    # the dict! (colors)\n",
        "    d = {1: (255, 175, 0),\n",
        "         2: (0, 255, 0),\n",
        "         3: (0, 0, 255)}\n",
        "\n",
        "    def reset(self):\n",
        "        # defining the food and player\n",
        "        self.player = Blob(self.SIZE)\n",
        "        self.food = Blob(self.SIZE)\n",
        "        while self.food == self.player:\n",
        "            self.food = Blob(self.SIZE) # when food$ player are in same point, re call food\n",
        "        \n",
        "        self.enemy = Blob(self.SIZE)\n",
        "        while self.enemy == self.player or self.enemy == self.food: # when food$ player are in same point, re call food\n",
        "            self.enemy = Blob(self.SIZE)\n",
        "\n",
        "        self.episode_step = 0\n",
        "\n",
        "        if self.RETURN_IMAGES:\n",
        "            observation = np.array(self.get_image())\n",
        "        else:\n",
        "            observation = (self.player-self.food) + (self.player-self.enemy)\n",
        "        return observation\n",
        "\n",
        "    def step(self, action): # When every step is proceeded\n",
        "        self.episode_step += 1 # increment an episode step\n",
        "        self.player.action(action) # player perfoms an action\n",
        "\n",
        "        #### MAYBE ###\n",
        "        #enemy.move()\n",
        "        #food.move()\n",
        "        ##############\n",
        "\n",
        "        if self.RETURN_IMAGES: # image of the participant location is returned at end\n",
        "            new_observation = np.array(self.get_image())\n",
        "        else: \n",
        "            new_observation = (self.player-self.food) + (self.player-self.enemy)\n",
        "\n",
        "        ## assigning rewards\n",
        "        if self.player == self.enemy:\n",
        "            reward = -self.ENEMY_PENALTY\n",
        "        elif self.player == self.food:\n",
        "            reward = self.FOOD_REWARD\n",
        "        else:\n",
        "            reward = -self.MOVE_PENALTY\n",
        "\n",
        "        done = False\n",
        "        if reward == self.FOOD_REWARD or reward == -self.ENEMY_PENALTY or self.episode_step >= 200:\n",
        "            done = True\n",
        "\n",
        "        return new_observation, reward, done\n",
        "\n",
        "\n",
        "    def render(self):\n",
        "        img = self.get_image()\n",
        "        img = img.resize((300, 300))  # resizing so we can see our agent in all its glory.\n",
        "        cv2.imshow(\"image\", np.array(img))  # show it!\n",
        "        cv2.waitKey(1)\n",
        "\n",
        "    # FOR CNN #\n",
        "    def get_image(self):\n",
        "        env = np.zeros((self.SIZE, self.SIZE, 3), dtype=np.uint8)  # starts an rbg of our size\n",
        "        env[self.food.x][self.food.y] = self.d[self.FOOD_N]  # sets the food location tile to green color\n",
        "        env[self.enemy.x][self.enemy.y] = self.d[self.ENEMY_N]  # sets the enemy location to red\n",
        "        env[self.player.x][self.player.y] = self.d[self.PLAYER_N]  # sets the player tile to blue\n",
        "        img = Image.fromarray(env, 'RGB')  # reading to rgb. Apparently. Even tho color definitions are bgr. ???\n",
        "        return img\n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLaNtl-qZZ1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = BlobEnv()\n",
        "\n",
        "# For stats\n",
        "ep_rewards = [-200]\n",
        "\n",
        "# For more repetitive results\n",
        "random.seed(1)\n",
        "np.random.seed(1)\n",
        "tf.random.set_seed(1)\n",
        "\n",
        "# Memory fraction, used mostly when trai8ning multiple agents\n",
        "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=MEMORY_FRACTION)\n",
        "#backend.set_session(tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)))\n",
        "\n",
        "# Create models folder\n",
        "if not os.path.isdir('models'):\n",
        "    os.makedirs('models')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOQ5_Xv7Asbl",
        "colab_type": "text"
      },
      "source": [
        "# The process routine "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJ3CbfAabjfU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "187ecebb-d271-4edc-9d45-ddef005c1ae3"
      },
      "source": [
        "agent = DQNAgent()\n",
        "\n",
        "# Iterate over episodes\n",
        "for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n",
        "\n",
        "    # Update tensorboard step every episode\n",
        "    agent.tensorboard.step = episode\n",
        "\n",
        "    # Restarting episode - reset episode reward and step number\n",
        "    episode_reward = 0\n",
        "    step = 1\n",
        "\n",
        "    # Reset environment and get initial state\n",
        "    current_state = env.reset() # rest- >  randomly place the food, player and target\n",
        "    # print(current_state.shape)\n",
        "\n",
        "    # Reset flag and start iterating until episode ends\n",
        "    done = False\n",
        "    while not done:\n",
        "\n",
        "        # This part stays mostly the same, the change is to query a model for Q values\n",
        "        if np.random.random() > epsilon:\n",
        "            # Get action from Q table\n",
        "            action = np.argmax(agent.get_qs(current_state))\n",
        "        else:\n",
        "            # Get random action\n",
        "            action = np.random.randint(0, env.ACTION_SPACE_SIZE)\n",
        "\n",
        "        new_state, reward, done = env.step(action)\n",
        "\n",
        "        # Transform new continous state to new discrete state and count reward\n",
        "        episode_reward += reward\n",
        "\n",
        "        if SHOW_PREVIEW and not episode % AGGREGATE_STATS_EVERY:\n",
        "            env.render()\n",
        "\n",
        "        # Every step we update replay memory and train main network\n",
        "        agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
        "        agent.train(done, step)\n",
        "\n",
        "        current_state = new_state\n",
        "        step += 1\n",
        "\n",
        "    # Append episode reward to a list and log stats (every given number of episodes)\n",
        "    ep_rewards.append(episode_reward)\n",
        "    if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
        "        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:])/len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
        "        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
        "        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
        "        agent.tensorboard.update_stats(reward_avg=average_reward, reward_min=min_reward, reward_max=max_reward, epsilon=epsilon)\n",
        "\n",
        "        # Save model, but only when min reward is greater or equal a set value\n",
        "        if min_reward >= MIN_REWARD:\n",
        "            agent.model.save(f'models/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')\n",
        "\n",
        "    # Decay epsilon\n",
        "    if epsilon > MIN_EPSILON:\n",
        "        epsilon *= EPSILON_DECAY\n",
        "        epsilon = max(MIN_EPSILON, epsilon)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/20000 [00:00<?, ?episodes/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(10, 10, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-e7f7d34f309b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mmin_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mAGGREGATE_STATS_EVERY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mmax_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mAGGREGATE_STATS_EVERY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_avg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_min\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward_max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Save model, but only when min reward is greater or equal a set value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-bd0f0d3ac7fa>\u001b[0m in \u001b[0;36mupdate_stats\u001b[0;34m(self, **stats)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Creates writer, writes custom metrics and closes writer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ModifiedTensorBoard' object has no attribute '_write_logs'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfPIuBxF-jBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}